# Note: If you change this file, please also change the file used for e2e tests!
# 
# https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/test/testdata/inferencepool-e2e.yaml

# --- ConfigMaps ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: latency-predictor-config
  namespace: default
data:
  LATENCY_RETRAINING_INTERVAL_SEC: "1"
  LATENCY_MIN_SAMPLES_FOR_RETRAIN: "100"
  LATENCY_TTFT_MODEL_PATH: "/models/ttft.joblib"
  LATENCY_TPOT_MODEL_PATH: "/models/tpot.joblib"
  LATENCY_TTFT_SCALER_PATH: "/models/ttft_scaler.joblib"
  LATENCY_TPOT_SCALER_PATH: "/models/tpot_scaler.joblib"
  LATENCY_MODEL_TYPE: "xgboost"
  LATENCY_MAX_TRAINING_DATA_SIZE_PER_BUCKET: "5000"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prediction-server-config
  namespace: default
data:
  LATENCY_MODEL_TYPE: "xgboost"
  PREDICT_HOST: "0.0.0.0"
  LOCAL_TTFT_MODEL_PATH: "/server_models/ttft.joblib"  # Use individual storage
  LOCAL_TPOT_MODEL_PATH: "/server_models/tpot.joblib"
  LOCAL_TTFT_SCALER_PATH: "/server_models/ttft_scaler.joblib"
  LOCAL_TPOT_SCALER_PATH: "/server_models/tpot_scaler.joblib"
---
# --- InferencePool ---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: vllm-llama3-8b-instruct
spec:
  targetPortNumber: 8000
  selector:
    app: vllm-llama3-8b-instruct
  extensionRef:
    name: vllm-llama3-8b-instruct-epp
---
# --- EPP Service ---
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
spec:
  selector:
    app: vllm-llama3-8b-instruct-epp
  ports:
    - name: epp-grpc
      protocol: TCP
      port: 9002
      targetPort: 9002
      appProtocol: http2
    - name: latency-predictor-training
      protocol: TCP
      port: 8000
      targetPort: 8000 
    - name: latency-predictor-1
      protocol: TCP
      port: 8001
      targetPort: 8001  
    - name: latency-predictor-2
      protocol: TCP
      port: 8002
      targetPort: 8002
    - name: latency-predictor-3
      protocol: TCP
      port: 8003
      targetPort: 8003
    - name: prometheus
      protocol: TCP
      port: 9090
      targetPort: 9090
  type: LoadBalancer 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
---
# --- EPP Deployment with Individual Container Volumes ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct-epp
  namespace: default
  labels:
    app: vllm-llama3-8b-instruct-epp
spec:
  replicas: 1  # Multiple EPP pods for scaling
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct-epp
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct-epp
    spec:
      serviceAccountName: vllm-llama3-8b-instruct-epp
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      # EPP Container
      - name: epp
        image:  us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/slo-routing-epp-exp
        imagePullPolicy: Always
        args:
        - -pool-name
        - "vllm-llama3-8b-instruct"
        - "-pool-namespace"
        - "default"
        - --pool-group
        - "inference.networking.x-k8s.io"
        - -v
        - "4"
        - --zap-encoder
        - "json"
        - -grpc-port
        - "9002"
        - -grpc-health-port
        - "9003"
        - "--config-file"
        - "/config/default-plugins.yaml"
        - "-enable-latency-predictor"
        env:
        - name: PREDICTION_SERVER_URL
          value: "http://localhost:8001,http://localhost:8002,http://localhost:8003"  # Multiple prediction servers
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"  # Single training server for sending training data
        - name: LATENCY_MAX_SAMPLE_SIZE
          value: "10000"  # Maximum sample size for latency prediction
        - name: NEG_HEADROOM_TPOT_WEIGHT
          value: "0.2"  # Weight for TPOT in negative headroom calculation
        - name: NEG_HEADROOM_TTFT_WEIGHT
          value: "0.8"  # Weight for TTFT in negative headroom calculation
        ports:
        - containerPort: 9002
        - containerPort: 9003
        - name: metrics
          containerPort: 9090
        livenessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      # Training Server Sidecar Container
      - name: training-server
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_training:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
          name: training-port
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8000
          initialDelaySeconds: 45
          periodSeconds: 10
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "4000m"
            memory: "8Gi"
        envFrom:
        - configMapRef:
            name: latency-predictor-config
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "training"
        volumeMounts:
        - name: training-server-storage
          mountPath: /models
      # Prediction Server Sidecar Container 1
      - name: prediction-server-1
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8001"]
        ports:
        - containerPort: 8001
          name: predict-port-1
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8001
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8001
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8001"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-1"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-1-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 2
      - name: prediction-server-2
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8002"]
        ports:
        - containerPort: 8002
          name: predict-port-2
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8002
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8002
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8002"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-2"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-2-storage
          mountPath: /server_models
      # Prediction Server Sidecar Container 3
      - name: prediction-server-3
        image: us-central1-docker.pkg.dev/benjaminbraun-gke-dev/slo-routing/latency_prediction:latest
        imagePullPolicy: Always
        command: ["uvicorn"]
        args: ["prediction_server:app", "--host", "0.0.0.0", "--port", "8003"]
        ports:
        - containerPort: 8003
          name: predict-port-3
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8003
          initialDelaySeconds: 15
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8003
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 10
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        envFrom:
        - configMapRef:
            name: prediction-server-config
        env:
        - name: PREDICT_PORT
          value: "8003"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_TYPE
          value: "prediction-3"
        - name: TRAINING_SERVER_URL
          value: "http://localhost:8000"
        volumeMounts:
        - name: prediction-server-3-storage
          mountPath: /server_models
      volumes:
      - name: training-server-storage
        emptyDir: 
          sizeLimit: "20Gi"  # Dedicated volume for training server
      - name: prediction-server-1-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 1
      - name: prediction-server-2-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 2
      - name: prediction-server-3-storage
        emptyDir: 
          sizeLimit: "10Gi"  # Dedicated volume for prediction server 3
      - name: plugins-config-volume
        configMap:
          name: plugins-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: plugins-config
  namespace: default
data:
  default-plugins.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: queue-scorer
    - type: kv-cache-utilization-scorer
    - type: slo-request-tracker
    - type: slo-scorer
    - type: slo-aware-profile-handler
    - type: max-score-picker
    schedulingProfiles:
    - name: default
      plugins:
      - pluginRef: slo-request-tracker
      - pluginRef: queue-scorer
      - pluginRef: kv-cache-utilization-scorer
      - pluginRef: max-score-picker
    - name: slo
      plugins:
      - pluginRef: slo-request-tracker
      - pluginRef: slo-scorer
      - pluginRef: max-score-picker
---
# --- RBAC ---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-read
  namespace: default
rules:
- apiGroups: [ "inference.networking.x-k8s.io" ]
  resources: [ "inferenceobjectives", "inferencepools" ]
  verbs: [ "get", "watch", "list" ]
- apiGroups: [ "inference.networking.k8s.io" ]
  resources: [ "inferencepools" ]
  verbs: [ "get", "watch", "list" ]
- apiGroups: [ "" ]
  resources: [ "pods" ]
  verbs: [ "get", "watch", "list" ]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-read-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: vllm-llama3-8b-instruct-epp
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-read
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auth-reviewer
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auth-reviewer-binding
subjects:
- kind: ServiceAccount
  name: vllm-llama3-8b-instruct-epp
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: auth-reviewer